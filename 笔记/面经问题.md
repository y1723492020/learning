####  1*1 卷积核的作用

##### 1. 实现跨通道的交互和信息整合

##### 2. 进行卷积核通道数的降维和升维 

​	1、降维（ dimension reductionality ）。比如，一张500 X500且厚度depth为100 的图片在20个filter上做1X1的卷积，那么结果的大小为500X500X20。 这是减少参数的一个常用的办法
​	 2、加入非线性。卷积层之后经过激励层，1X1的卷积在前一层的学习表示上添加了非线性激励（ non-linear activation ），提升网络的表达能力；

![1572847085496](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1572847085496.png)

##### 3.可以在保持feature map 尺寸不变（即不损失分辨率）的前提下大幅增加非	线性特性



#### 防止过拟合几种常见的方法

在对模型进行训练时，有可能遇到训练数据不够，即训练数据无法对整个数据的分布进行估计的时候，或者在对模型进行过度训练（overtraining）时，常常会导致模型的过拟合（overfitting）。如下图所示：

![1572847420320](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1572847420320.png)

通过上图可以看出，随着模型训练的进行，模型的复杂度会增加，此时模型在训练数据集上的训练误差会逐渐减小，但是在模型的复杂度达到一定程度时，模型在验证集上的误差反而随着模型的复杂度增加而增大。此时便发生了过拟合，即模型的复杂度升高，但是该模型在除训练集之外的数据集上却不work。 

为了防止过拟合，我们需要用到一些方法，如：**early stopping、数据集扩增（Data augmentation）、正则化（Regularization）、Dropout**等。

##### 1. early stopping 

对模型进行训练的过程即是对模型的参数进行学习更新的过程，这个参数学习的过程往往会用到一些迭代方法，如梯度下降（Gradient descent）学习算法。Early stopping便是一种迭代次数截断的方法来防止过拟合的方法，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合。 

Early stopping方法的具体做法是，在每一个Epoch结束时（一个Epoch集为对所有的训练数据的一轮遍历）计算validation data的accuracy，当accuracy不再提高时，就停止训练。这种做法很符合直观感受，因为accurary都不再提高了，在继续训练也是无益的，只会提高训练的时间。那么该做法的一个重点便是怎样才认为validation accurary不再提高了呢？**并不是说validation accuracy一降下来便认为不再提高了**，因为可能经过这个Epoch后，accuracy降低了，但是随后的Epoch又让accuracy又上去了，所以不能根据一两次的连续降低就判断不再提高。**一般的做法是，在训练的过程中，记录到目前为止最好的validation accuracy，当连续10次Epoch（或者更多次）没达到最佳accuracy时，则可以认为accuracy不再提高了。此时便可以停止迭代了（Early Stopping）。这种策略也称为“No-improvement-in-n”，n即Epoch的次数，可以根据实际情况取，如10、20、30……**

##### 2.  数据集扩增 

在数据挖掘领域流行着这样的一句话，“有时候往往拥有更多的数据胜过一个好的模型”。因为我们在使用训练数据训练模型，通过这个模型对将来的数据进行拟合，而在这之间又一个假设便是，训练数据与将来的数据是独立同分布的。即使用当前的训练数据来对将来的数据进行估计与模拟，而更多的数据往往估计与模拟地更准确。因此，更多的数据有时候更优秀。但是往往条件有限，如人力物力财力的不足，而不能收集到更多的数据，如在进行分类的任务中，需要对数据进行打标，并且很多情况下都是人工得进行打标，因此一旦需要打标的数据量过多，就会导致效率低下以及可能出错的情况。所以，往往在这时候，需要采取一些计算的方式与策略在已有的数据集上进行手脚，以得到更多的数据。 

通俗得讲，数据机扩增即需要得到更多的符合要求的数据，即和已有的数据是独立同分布的，或者近似独立同分布的。一般有以下方法：

- 从数据源头采集更多数据
- 复制原有数据并加上随机噪声
- 重采样
- 根据当前数据集估计数据分布参数，使用该分布产生更多数据等

##### 3. 正则化方法

正则化方法是指在进行目标函数或代价函数优化时，在目标函数或代价函数后面加上一个正则项，一般有L1正则与L2正则等。

##### 4. Dropout 方法

正则是通过在代价函数后面加上正则项来防止模型过拟合的。而在神经网络中，有一种方法是通过修改神经网络本身结构来实现的，其名为Dropout。该方法是在对网络进行训练时用一种技巧（trick），对于如下所示的三层人工神经网络： 

![1572848028865](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1572848028865.png)

对于上图所示的网络，在训练开始时，随机得删除一些（可以设定为一半，也可以为1/3，1/4等）隐藏层神经元，即认为这些神经元不存在，同时保持输入层与输出层神经元的个数不变，这样便得到如下的ANN：

![1572848078488](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1572848078488.png)

然后按照BP学习算法对ANN中的参数进行学习更新（虚线连接的单元不更新，因为认为这些神经元被临时删除了）。这样一次迭代更新便完成了。下一次迭代中，同样随机删除一些神经元，与上次不一样，做随机选择。这样一直进行瑕疵，直至训练结束。 （ANN是指由大量的处理单元(神经元) 互相连接而形成的复杂网络结构，是对人脑组织结构和运行机制的某种抽象、简化和模拟。 [人工神经网络](https://baike.baidu.com/item/人工神经网络/382460)（Artificial Neural Network，简称ANN ））

Dropout方法通过修改人工神经网络中隐藏层的神经元的个数（随机删除一些神经元，下次迭代可不同）来防止人工神经网络的过拟合。例如，在训练时，**每次**随机（如 50% 概率）忽略隐层的某些节点。

#### Dropout一般放在哪些位置

Dropout 层一般加在全连接层 防止过拟合 提升模型泛化能力。
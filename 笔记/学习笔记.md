# ROC曲线及其阈值的计算设置、AUC值

学习内容来自于博客： https://blog.csdn.net/zdy0_2004/article/details/44948511

1、roc曲线：接收者操作特征(receiveroperating characteristic),roc曲线上每个点反映着对同一信号刺激的感受性。

2针对一个二分类问题，将实例分成正类(postive)或者负类(negative)。但是实际中分类时，会出现四种情况.

**TP**:正确的肯定数目

**FN**:漏报，没有找到正确匹配的数目

**FP**:误报，没有的匹配不正确

**TN**:正确拒绝的非匹配数目

(1)真正类率(True Postive Rate)TPR: **TP/(TP+FN)**,代表分类器预测的**正类中**实际正实例占所有正实例的比例。Sensitivity

(2)负正类率(False Postive Rate)FPR: **FP/(FP+TN)**，代表分类器预测的**正类中**实际负实例占所有负实例的比例。1-Specificity

(3)真负类率(True Negative Rate)TNR: **TN/(FP+TN)**,代表分类器预测的**负类中**实际负实例占所有负实例的比例，**TNR=1-FPR**。Specificity

![1572935660237](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1572935660237.png)

横轴FPR:1-TNR,1-Specificity，FPR越大，预测正类中实际负类越多。

纵轴TPR：Sensitivity(正类覆盖率),TPR越大，预测正类中实际正类越多。

理想目标：TPR=1，FPR=0,**即图中(0,1)点，故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好，**Sensitivity、Specificity**越大效果越好。**

假设采用逻辑回归分类器，其给出针对每个实例为正类的概率，那么通过设定一个阈值如0.6，概率大于等于0.6的为正类，小于0.6的为负类。对应的就可以算出一组(FPR,TPR),在平面中得到对应坐标点。随着阈值的逐渐减小，越来越多的实例被划分为正类，但是这些正类中同样也掺杂着真正的负实例，即TPR和FPR会同时增大。阈值最大时，对应坐标点为(0,0),阈值最小时，对应坐标点(1,1)。如下例子所示，

**如何画**roc**曲线**

假设已经得出一系列样本被划分为正类的概率，然后按照大小排序，下图是一个示例，图中共有20个测试样本，“Class”一栏表示每个测试样本真正的标签（p表示正样本，n表示负样本），“Score”表示每个测试样本属于正样本的概率。

![1572935753392](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1572935753392.png)

接下来，我们从高到低，依次**将“Score”值作为阈值threshold**，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本。举例来说，对于图中的第4个样本，其“Score”值为0.6，那么样本1，2，3，4都被认为是正样本，因为它们的“Score”值都大于等于0.6，而其他样本则都认为是负样本。**每次选取一个不同的threshold**，我们就可以得到一组FPR和TPR，即ROC曲线上的一点。这样一来，我们一共得到了20组FPR和TPR的值，将它们画在ROC曲线的结果如下图：

![1572935934912](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1572935934912.png)

**AUC**

如果多个模型有多个roc曲线，如下图中的三条曲线（对应三个模型）相互交叉，并不容易区分哪个模型更优，于是就引出了一个新的指标：我们将曲线与x轴、直线x=1围成的面积（即曲线下方的面积），称作AUC（Area under the curve）。AUC位于0到1之间，取值越大说明模型越好

![1572936066421](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1572936066421.png)

**首先AUC值是一个概率值，当你随机挑选一个正样本以及负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值，AUC值越大，当前分类算法越有可能将正样本排在负样本前面，从而能够更好地分类。**

**三 为什么使用Roc和Auc评价分类器**

既然已经这么多标准，为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变换的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现样本类不平衡，即正负样本比例差距较大，而且测试数据中的正负样本也可能随着时间变化。下图是ROC曲线和Presision-Recall曲线的对比：

![1572936410227](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1572936410227.png)

在上图中，(a)和(c)为Roc曲线，(b)和(d)为Precision-Recall曲线。

(a)和(b)展示的是分类其在原始测试集(正负样本分布平衡)的结果，(c)(d)是将测试集中负样本的数量增加到原来的10倍后，分类器的结果，可以明显的看出，ROC曲线基本保持原貌，而Precision-Recall曲线变化较大。



# 卷积神经网络模型参数量和运算量计算方法

搬运工 ： https://www.jianshu.com/p/d4db25322435

1.对CNN而言，每个卷积层的**参数量**计算如下：

![1573181032972](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1573181032972.png)

2.对CNN而言，每个卷积层的**运算量**计算如下：

![1573181099472](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1573181099472.png)

3.对全连接层而言，其参数量非常容易计算：

![1573181119381](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1573181119381.png)

4.对全连接层而言，其运算量计算如下：

![1573181141614](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1573181141614.png)

# Batch Normalization

**解决了我的困惑**，BN是干什么的？ 

**机器学习领域有个很重要的假设：IID独立同分布假设，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。而BatchNorm是干啥的呢？BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。**

为什么深度神经网络随着网络深度加深，训练起来越困难，收敛越来越慢？这是个在DL领域很接近本质的好问题。很多论文都是解决这个问题的，比如ReLU激活函数，再比如Residual Network，BN本质上也是解释并从某个不同的角度来解决这个问题的。

论文首先说明Mini-Batch SGD相对于One Example SGD的两个优势：梯度更新方向更准确；并行计算速度快；（本文作者：为啥要说这些？因为BatchNorm是基于Mini-Batch SGD的，所以先夸下Mini-Batch SGD，当然也是大实话）；

但是缺点：SGD训练的缺点：超参数调起来很麻烦。（隐含意思是用我大BN就能解决很多SGD的缺点：用了大BN，妈妈再也不用担心我的调参能力啦）



### 我们为什么需要BN

我们知道网络一旦train起来，那么参数就要发生更新，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。我们把网络中间层在训练过程中，数据分布的改变称之为：“Internal Covariate Shift”。BN的提出，就是要解决在训练过程中，中间层数据分布发生改变的情况。

BN 被添加在全连接层（仿射层）与  激励函数之间

![1573181681861](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1573181681861.png)

上面红色区域的的信息能更敏感更有效的向前传递信息， 而![1573181727437](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1573181727437.png)这个红色区域就对x变化，y变化就不敏感了，BN的作用就是使数据分布在有效的区域内。

![1573181884673](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1573181884673.png)

上面的图就是加了BN与没加BN的数据分布对比图，可以看出，没有加BN的数据，经过激励函数之后，数据大多分布在1，或者-1 这两个不敏感的区域， 而经过BN的数据再经过tanh激励， 数据在整个区间都有分布。

BN算法公式 ：

![1573182124100](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1573182124100.png)

其中前三步就是BN的标准化步骤，而第四步就是将前面BN操作后的数据进行尺度变换和平移，为了神经 网络自己能使用，扩展和学习参数γ，和平移参数β，其中γ和β两个参数就是神经网络学习判断BN是否起到优化作用，如果没有起到作用，就用γ和β来抵消BN的一些操作。

尺度变换和偏移：将![x_i](https://math.jianshu.com/math?formula=x_i)乘以![\gamma](https://math.jianshu.com/math?formula=%5Cgamma)调整数值大小，再加上![\beta](https://math.jianshu.com/math?formula=%5Cbeta)增加偏移后得到![y_i](https://math.jianshu.com/math?formula=y_i)，这里的![\gamma](https://math.jianshu.com/math?formula=%5Cgamma)是尺度因子，![\beta](https://math.jianshu.com/math?formula=%5Cbeta)是平移因子。这一步是BN的精髓，由于归一化后的![x_i](https://math.jianshu.com/math?formula=x_i)基本会被限制在正态分布下，使得网络的表达能力下降。为解决该问题，我们引入两个新的参数：![\gamma](https://math.jianshu.com/math?formula=%5Cgamma)，![\beta](https://math.jianshu.com/math?formula=%5Cbeta)。 ![\gamma](https://math.jianshu.com/math?formula=%5Cgamma)和![\beta](https://math.jianshu.com/math?formula=%5Cbeta)是在训练时网络自己学习得到的，**如果![\gamma = \sqrt{\sigma^2 + \epsilon}](https://math.jianshu.com/math?formula=%5Cgamma%20%3D%20%5Csqrt%7B%5Csigma%5E2%20%2B%20%5Cepsilon%7D)，而![\beta = \mu](https://math.jianshu.com/math?formula=%5Cbeta%20%3D%20%5Cmu)，那么就会恢复到之前的原数据**。


下面是神经网络训练到最后代表每层结果的分布图：

![1573182174630](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1573182174630.png)

让每一层在有效的范围内传递下去就是Batch Normalization 的最大好处（还有反向标准化）

**batch**理论上是将整体数据分为均匀的几块batch，然后每个batch再分别的进行运算处理，如下图 training stage (训练阶段如下)

![1573195334997](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1573195334997.png)


$$
但是实际上  gpu 运行的时候将 x^1 、 x^2 、 x^3 的三个matrix 合并在一块，一起乘上 W^1
$$
​           达到**gpu加速**的效果，如下图所示：

![1573195623240](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1573195623240.png)



其中**反标准化工序**的训练过程如下：

![1573198524007](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1573198524007.png)



# 准确率与召回率的意义与区别

对于准确率和召回率：一句话，**准确率就是“找的对”，召回率就是“找的全”**

参考下图

![1573629821749](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1573629821749.png)

区域A：能检索到；想得到

区域B：能检索到；不想得到

区域C：未检索到；想得到

区域D：未检索到；不想得到

注：准确率和召回率互相影响，理想状态下肯定追求两个都高，但是实际情况是两者相互“制约”：追求准确率高，则召回率就低；追求召回率高，则通常会影响准确率。若两者都低，则一般是出了某种问题

一组不同阈值下，准确率和召回率的关系如下图：

![1573629866500](C:\Users\CBSR_YXG\AppData\Roaming\Typora\typora-user-images\1573629866500.png)

实际应用：

（1）如果是做搜索，则要在保证召回率理想的情况下，提升准确率；

（2）如果做疾病监测、分类垃圾，则是要保证准确率的条件下，提升召回率。






